# Statistical Modeling: The Two Cultures

> Author: Breiman, 2001
>
> Link: https://projecteuclid.org/euclid.ss/1009213726

Contribution: 

* This is more of a philosophical paper that I thought would be interesting to read. Basically, Breiman argues that statistics as a field should be less dependent on traditional data models and be more open to machine learning models (what he calls algorithmic modeling).

Thoughts:

* I would say that many of the arguments Breiman makes is more widely accepted today in the statistics community. But it's also interesting to see that not much has changed. The models, type of data (e.g. microarray), and problems he talks about still define the field today (almost 20 years later!).
* He does a good job of concisely pointing out three problems, which he refers to as Rashomon (stability of your model), Occam (simplicity vs accuracy), and Bellman (dimensionality). 
* I think some drawbacks of an algorithmic/machine learning approach is more evident today. Infamously, Amazon created a [hiring algorithm](https://www.aclu.org/blog/womens-rights/womens-rights-workplace/why-amazons-automated-hiring-tool-discriminated-against) that they later discovered discriminated against women. A reliance on machine learning algorithms, typically opaque and harder to interpret, may make it easier to overlook/miss such problems.